\section{From Binary Join to Free Join}\label{sec:background}

\begin{figure}
  \begin{align*}
    Q_{\clubsuit}(x,a,b,c) \cd R(x,a),S(x,b),T(x,c).
  \end{align*}
  \begin{center}
    \begin{tabular}{c}
      \begin{lstlisting}[language=SQL, numbers=none]
SELECT * FROM R, S, T WHERE R.x = S.x AND S.x = T.x
\end{lstlisting}
    \end{tabular}
  \end{center}
  \begin{alignat*}{8}
    R & = & \set{(x_0, a_0)} & \cup & \{(x_1, a_i^l), & (x_2, a_i^r) &  & \mid i \in [1 \ldots n] & \} \\
    S & = & \set{(x_0, b_0)} & \cup & \{(x_2, b_i^l), & (x_3, b_i^r) &  & \mid i \in [1 \ldots n] & \} \\
    T & = & \set{(x_0, c_0)} & \cup & \{(x_3, c_i^l), & (x_1, c_i^r) &  & \mid i \in [1 \ldots n] & \}
  \end{alignat*}
  \caption{The clover query $Q_\clubsuit$, and an input instance.
    Note that $x_0$ is the only $x$-value in all three relations,
    therefore the only output tuple is $(x_0, a_0, b_0, c_0)$. }
  \label{fig:clover-query}
\end{figure}

In this section we introduce the \FJ framework.
Unlike our SIGMOD paper~\cite{10.1145/3589295} which defines \FJ from
the basic building blocks,
here we start from the traditional binary join
and gently massage it into the more general \FJ.
To keep the presentation intuitive,
we will be following an example instead of
defining the algorithm in full generality.
We refer the reader to our SIGMOD paper~\cite{10.1145/3589295}
for a more formal treatment.

\subsection{Basic Concepts and Notations}\label{sec:basic-concepts}

For simplicity we consider only {\em natural join} queries,
where all joins are equijoins, and all input relations
are joined over common attributes.
Such queries are also known as {\em conjunctive queries}
and can be written in ``Datalog notation'' as the following
example shows.

\begin{example} \label{ex:triangle}
  Consider SQL query in Figure~\ref{fig:clover-query}.
  The corresponding conjunctive query appears above it,
  where each of $R(x, a)$, $S(x, b)$, and $T(x, c)$
  is called a {\em body atom}, and $Q_\clubsuit(x, a, b, c)$
  the {\em head atom}.
\end{example}

It is often convenient to view a conjunctive query as
a hypergraph.  The \emph{query hypergraph} of $Q$ consists of vertices
$\mathcal{V}$ and edges $\mathcal{E}$, where the set of nodes
$\mathcal{V}$ is the set of variables occurring in $Q$, and the set of
hyperedges $\mathcal{E}$ is the set of body atoms in $Q$.
The hypergraph for $Q_\clubsuit$ has four vertices, each for
$x$, $a$, $b$, and $c$,
and three edges, each for $R(x, a)$, $S(x, b)$, and $T(x, c)$.
As standard, we
say that the query $Q$ is {\em acyclic} if its associated hypergraph
is $\alpha$-acyclic\footnote{The reader does not need to be familiar
  with definitions of acyclic queries to understand \FJ.}~\cite{DBLP:journals/jacm/Fagin83}.
Note that $Q_\clubsuit$ is acyclic,
while an example of a {\em cyclic} query is the ``triangle query'':
$$Q_\triangle(x, y, z) \cd U(x, y), V(y, z), W(z, x).$$
whose query hypergraph is a triangle.

We now introduce a notation to make pseudocode cleaner.
Inside a loop,
we will write \lstinline|m = M[x]?|
for looking up $x$ from the hash map $M$;
if $M$ contains $x$, we assign the result of the lookup
to $m$; otherwise, we \lstinline|continue| to the next iteration
of the enclosing loop.
In other words, the code fragments in Figure~\ref{fig:lookup-notation}
are equivalent.
\begin{figure}
  \begin{subfigure}[t]{0.5\linewidth}
    \begin{lstlisting}
for ...:
  m = M[x]?
  ...
\end{lstlisting}
  \end{subfigure}
  \begin{subfigure}[t]{.45\linewidth}
    \begin{lstlisting}
for ...:
  if x not in M: 
    continue
  else: 
    m = M[x]
    ...
\end{lstlisting}
  \end{subfigure}
  \caption{Example using the notation \lstinline|m = M[x]?|.
    The two code fragments are equivalent.}
  \label{fig:lookup-notation}
\end{figure}

% \subsection{Basic Concepts}\label{sec:basic-concepts}

% For simplicity we consider only {\em natural join} queries,
% where all joins are equijoins, and all input relations
% are joined over common attributes.
% Such queries are also known as {\em conjunctive queries}
% and can be written in ``Datalog notation'' as the following
% example shows.

% \begin{example} \label{ex:triangle} Consider the following SQL query:
%   \begin{lstlisting}[language=SQL]
% -- Schema: R(x,y), S(y,z), T(z,x)
% SELECT R.x, S.y, T.z FROM R, S, T 
%  WHERE R.y = S.y AND S.z = T.z AND T.x = R.x
% \end{lstlisting}
%   %
%   The corresponding conjunctive query is:
%   $$Q_{\triangle}(x,y,z) \cd R(x, y), S(y,z), T(z, x).$$
%   %
%   where each of $R(x, y)$, $S(y,z)$, and $T(z, x)$
%   is called a {\em body atom}, and $Q_\triangle(x, y, z)$
%   the {\em head atom}.
% \end{example}

% It is often convenient to view a conjunctive query as
% a hypergraph.  The \emph{query hypergraph} of $Q$ consists of vertices
% $\mathcal{V}$ and edges $\mathcal{E}$, where the set of nodes
% $\mathcal{V}$ is the set of variables occurring in $Q$, and the set of
% hyperedges $\mathcal{E}$ is the set of body atoms in $Q$.
% The hypergraph for $Q_\triangle$ is a triangle with three
% vertices $x, y$, and $z$ and three edges corresponding to
% $R(x, y)$, $S(y,z)$, and $T(z, x)$.
% For this reason we will refer to $Q_\triangle$ as the {\em triangle
%     query}.
% As standard, we
% say that the query $Q$ is {\em acyclic} if its associated hypergraph
% is $\alpha$-acyclic\footnote{The reader does not need to be familiar
%   with definitions of acyclic queries to understand \FJ.}~\cite{DBLP:journals/jacm/Fagin83}.


\subsection{Binary Join}\label{sec:binary-join}

\begin{figure*}
  \begin{subfigure}[t]{0.21\linewidth}
    \begin{lstlisting}[showlines=true]
for (x,a) in R:
  s = S[x]?
  for (x,b) in s:
    t = T[x]?
    for (x,c) in t:
      output(x,a,b,c)




\end{lstlisting}
    \caption{Binary \FJ.}
    \label{fig:bj-loop}
  \end{subfigure}
  \begin{subfigure}[t]{0.27\linewidth}
    \centering
    \begin{lstlisting}[showlines=true, numbers=none]
for i in 0..R.len():
  x = R.x[i]; a = R.a[i]
  s = S[x]? # s:x->[j]
  for j in s:
    x = S.x[j]; b = S.b[j]
    t = T[x]? # t:x->[k]
    for k in t:
      x = T.x[k]; c = T.c[k]
      output(x,a,b,c)

\end{lstlisting}
    \caption{Columnar storage.}
    \label{fig:column-loop}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\linewidth}
    \centering
    \begin{lstlisting}[escapeinside={(*}{*)}]
for i in 0..R.len():
  x = R.x[i]; s = S[x]?
  for j in s:
   (* \underline{x = S.x[j];} *)t = T[x]?
    for k in t:
     (* \underline{x = T.x[k]} *)
      a = R.a[i]
      b = S.b[j]
      c = T.c[k]
      output(x,a,b,c)
\end{lstlisting}
    \caption{Late materialization.}
    \label{fig:late-materialization}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\linewidth}
    \centering
    \begin{lstlisting}[
    showlines=true,
    numbers=none
]
for i in 0..R.len():
  x = R.x[i]; 
  s = S[x]?; t = T[x]?
  for j in s:
    for k in t:
      a = R.a[i]
      b = S.b[j]
      c = T.c[k]
      output(x,a,b,c)

\end{lstlisting}
    \caption{Late iteration.}
    \label{fig:factorized-loop}
  \end{subfigure}
  \caption{Execution of binary join for the clover query~\ref{fig:bj-loop},
    and three transformations.
    The first transformation~\ref{fig:column-loop} makes the algorithm work
    on column-wise storage instead of a row-wise one;
    the second transformation~\ref{fig:late-materialization} performs
    the classic late materialization optimization;
    the last one is another transformation that we call
    late iteration~\ref{fig:factorized-loop}.
  }
\end{figure*}

The standard approach to computing a natural join of multiple relations is
to compute one binary join at a time.  A {\em binary plan} is a binary
tree, where each internal node is a join operator $\Join$, and each
leaf node is one of the base tables $R_i$.
The plan is a \emph{left-deep linear plan}, or
simply left-deep plan, if the right child of every join is a leaf
node.  If the plan is not left-deep, then we call it \emph{bushy}.
For example, $(R \Join S) \Join (T \Join U)$ is a bushy plan, while
$((R \Join S) \Join T) \Join U$ is a left-deep plan.  We do not treat
specially right-deep or zig-zag plans, but simply consider them to be
bushy.

In this paper we consider only hash-joins, which are the most
common types of joins in database systems.
The standard way to execute a bushy plan is to
decompose it into a series of left-deep linear plans.  Every join node
that is a right child becomes the root of a new subplan, which is
first evaluated, and its result materialized, before the parent join
can proceed.  As a consequence, every binary plan, bushy or not,
becomes a collection of left-deep plans. We decompose bushy
plans in exactly the same way, and we will focus on left-deep linear
plans in the rest of this paper.  For example, the bushy plan
$(R \Join S) \Join (T \Join U)$ is converted into two plans:
$P_1 = T \Join U$ and $P_2 = (R \Join S) \Join P_1$; both are
left-deep plans.

To reduce clutter, we represent a left-deep plan
$(\cdots ((R_1 \Join R_2) \Join R_3) \cdots \Join R_{m-1}) \Join R_m$
as $[R_1, R_2, \ldots, R_m]$.  Evaluation of a left-deep plan is done
using pipelining.  The engine iterates over each tuple in the
left-most base table $R_1$; each tuple is probed in $R_2$; each of the
matching tuple is further probed in $R_3$, etc.


\begin{example}
  A possible left-deep linear plan for $Q_\clubsuit$ is $[R, S, T]$,
  which represents $\left(R(x,a) \Join S(x,b)\right) \Join T(x,c)$.  To execute
  this plan, we first build a hash table for $S$ keyed on $x$,
  where each $x$ maps to a vector of $(x,y)$ tuples,
  and a hash table for $T$ keyed on $x$, each mapped to a
  vector of $(x,c)$ tuples\footnote{When the relations are bags, then
    the hash table may contain duplicate tuples, or store separately
    the multiplicity.  We also note that the question what exactly to
    store in the hash table (e.g. copies of the tuples, or pointers to
    the tuple in the buffer pool) has been studied for a long time,
    see~\cite{DBLP:journals/csur/Graefe93}.}.
  Then the execution
  proceeds as shown in Figure~\ref{fig:bj-loop}.
  For each tuple $(x, a)$ in $R$, we first probe into the hash table for $S$
  using $x$ to get a vector of $(x, b)$ tuples.  We then loop over
  each $(x, y)$ and probe into the hash table for $T$ with $x$.
  Each successful probe will return a vector of $(x, c)$ tuples,
  and we output the tuple $(x, a, b, c)$ for each $(x, c)$.
\end{example}

\subsection{Columnar Storage and Late Materialization}\label{sec:late-materialization}
The first transformation we perform on the binary join algorithm
makes it work on column-wise storage instead of a row-wise one.
This is not yet an optimization because it likely will not improve the
performance, but this step serves as an important bridge to the next
optimizations.
As Figure~\ref{fig:column-loop} shows, in the outermost loop we
iterate over row indices instead of tuples.
For each row index $i$, we retrieve the $x$-value $R.x[i]$,
as well as the corresponding $a$-value $R.a[i]$.
The hash maps for $S$ and $T$ now map each $x$ to a vector of row indices,
so we next look up into the hash map for $S$ using $x$
to get a vector of $j$.
In the second loop, we retrieve the $x$-value and $b$-value
from $S$ for each $j$,
then probe into $T$ to get a vector of $k$.
Finally, we retrieve the $x$-value and $c$-value from $T$ for each $k$,
and output the tuple $(x, a, b, c)$.

A key inefficiency of the algorithm in Figure~\ref{fig:column-loop} is that,
although the query only outputs a single tuple $(x_0, a_0, b_0, c_0)$,
we still did a lot of work retrieving the different, $a$, $b$, and $c$ values
from their respective columns.
For example, since we iterate over the entire $R$ relation,
we retrieve all $2n+1$ $a$-values from $R.a$;
even worse, since $|R \bowtie S| = n^2 + 1$,
we will access $S.b[j]$ $O(n^2)$ times.
A better strategy is to {\em delay} the retrieval of these values
until we actually need them.
In this case, we can delay the retrieval of $a$, $b$, and $c$
until we are ready to output the tuple $(x, a, b, c)$.
This way we only need to access each of $R.a$, $S.b$, and $T.c$ once,
instead of $O(n^2)$ times.
This is precisely the classic {\em late materialization}
optimization~\cite{DBLP:conf/icde/AbadiMDM07} now implemented in
nearly all modern database systems.

\subsection{Late Iteration and Free Join}\label{sec:late-materialization}
We can go one step beyond late materialization and further optimize
the code in Figure~\ref{fig:late-materialization}.
The key observation is that, although we retrieve $x$ from the
different relations in each loop level,
{\em they have to be the same value} because $x$ is the join attribute!
This means we can first the last two redundant retrievals of $x$
and reuse the value from the outermost loop,
which corresponds to removing the underlined code
in Figure~\ref{fig:late-materialization}.
At this point, we can see that the remaining body of the second loop,
\lstinline|t = T[x]?|,
does not depend on the loop variable $j$ at all.
We can therefore pull the lookup out of the loop,
resulting in the code in Figure~\ref{fig:factorized-loop}.
In other words, we {\em delay} the iteration over $s$
until after the lookup on $T$ succeeds.

Note that this final optimization has improved the asymptotic
run time of the algorithm:
although late materialization already reduces a quadratic
number of access to the relation columns,
it still needs to iterate over $O(n^2)$ row indices,
because the first two loop levels essentially compute
the join $R \bowtie S$;
in contrast, the first loop level in Figure~\ref{fig:factorized-loop}
joins every tuple of $R$ with $S$ and $T$ at the same time,
and the entire algorithm now runs in $O(n)$ time.

At the moment, our optimizations may appear rather low-level and ad-hoc.
Taking a step back, we can understand the execution of
any join algorithm as a series of iterations and lookups.
The transformation from row-wise to column-wise storage
involves changing {\em what to iterate over} (row indices instead of tuples);
the late materialization optimization changes {\em what to look up}
and {\em when to look up} (look up row indices first, then retrieve values later);
finally, the late iteration optimization reorders the iterations and lookups.

While columnar storage and late materialization have become stables of
modern database systems,
the contribution of the \FJ framework is a new abstraction
to describe the ordering of iterations and lookups
that we call the \FJ plan.
The basic building blocks of a \FJ plan are
called {\em subatoms}, each of which is a subset of a relation schema.
%
\begin{definition}
  Given a relation schema $R(x_1, x_2, \ldots)$,
  a \emph{subatom} of $R$ is of the form $R(v_1, v_2, \ldots)$
  where $\set{v_1, v_2, \ldots} \subseteq \set{x_1, x_2, \ldots}$
\end{definition}
%
For example, given the relation $R$ with schema $R(x, y)$,
all of the following are valid subatoms: $R(), R(x), R(y), R(x, y)$.
A \FJ plan over a set of schemas is a sequence of groups,
where each group is a list of subatoms.
%
\begin{definition}
  Given a join query $Q$ over $R_1, R_2, \ldots$,
  a \FJ plan for $Q$ is of the form $[R_i(\mathbf{x}_i), R_j(\mathbf{x_j}), \ldots],
    [R_k(\mathbf{x_k}), \ldots], \ldots$ where each of
  $R_i(\mathbf{x_i}), R_j(\mathbf{x_j}), R_k(\mathbf{x_k}), \ldots$
  is a subatom over the schema of $R_i, R_j, R_k, \ldots$ respectively.
\end{definition}
%
Each group in a \FJ plan corresponds to a loop level.
At each loop level,
we iterate over tuples of the first subatom in the group,
and use the values to look up into the remaining subatoms.
For this reason, we will sometimes stylize a \FJ plan as follows
to emphasize the iterated subatom and reflect the loop nesting:
\begin{alignat*}{4}
   & [R_1        &  & (\mathbf{x_1})\mid R_2(\mathbf{x_2}), R_3(\mathbf{x_3}), \ldots] \\
   & \rightarrow &  & [R_4(\mathbf{x_4})\mid R_5(\mathbf{x_2}), \ldots]                \\
   &             &  & \rightarrow [R_6(\mathbf{x_6})\mid \ldots]
\end{alignat*}
%
\begin{example}
  The \FJ plan for the algorithm in Figure~\ref{fig:factorized-loop}
  is:
  \begin{alignat*}{4}
     & [R(         &  & x, a) \mid S(x), T(x)]   \\
     & \rightarrow &  & [S(b) \mid ]             \\
     &             &  & \rightarrow [T(c) \mid ]
  \end{alignat*}
  Although we only retrieve the value of $a$ in the innermost loop,
  eacho $a$-value one-to-one corresponds to each $i$,
  so the plan iterates over both $x$ and $a$ at the first level.
\end{example}
%
\begin{example}
  We can represent the binary join algorithm in Figure~\ref{fig:bj-loop}.
  with the \FJ plan:
  \begin{alignat*}{4}
     & [R(         &  & x, a) \mid S(x)]        \\
     & \rightarrow &  & [S(b) \mid T(x)]        \\
     &             &  & \rightarrow [T(c) \mid]
  \end{alignat*}
  if we ignore the redundant $x$ at the inner loop levels.
\end{example}
%
In face, every left-linear binary join plan $[R_1, R_2, R_3, \ldots]$
can be represented by a \FJ plan:
\begin{alignat*}{4}
   & [R_1(       &  & \mathbf{x_1}) \mid &  & R_2(\mathbf{\mathbf{x_1} \cap \mathbf{x_2}})]                                      \\
   & \rightarrow &  & [R_2(              &  & \mathbf{x_2} \setminus \mathbf{x_1}) \mid R_3(\mathbf{x_2}\cap \mathbf{x_3})]      \\
   &             &  & \rightarrow        &  & [R_3(\mathbf{x_3} \setminus \mathbf{x_2}) \mid R_4(\mathbf{x_3}\cap \mathbf{x_4})] \\
   &             &  &                    &  & \vdots
\end{alignat*}
%
As we will see in Section~\ref{sec:other-optimizations},
we can also represent any \GJ algorithm with a \FJ plan.
\FJ therefore generalizes and unifies both binary join and \GJ.
However, the true power of \GJ is its ability to represent
algorithms like the one in Figure~\ref{fig:factorized-loop}
that are neither binary join nor \GJ,
leading to performance improvements beyond the existing algorithms.

\begin{figure*}
  \begin{subfigure}[t]{0.23\linewidth}
    \begin{lstlisting}
# [ R(x, y) | S(y) ]
# -> [ S(z) | T(z, x) ]

for (x,y) in R:
  s = S[y]? # S:y->[z]
  for z in s:
    if (z,x) in T:
      output(x,y,z)
\end{lstlisting}
    \caption{Plan equivalent to binary join.}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\linewidth}
    \begin{lstlisting}[numbers=none, showlines=true]
# [ R(x, y) | S(y), T(x) ]
# -> [ S(z) | T(z) ]

for (x,y) in R:
  s = S[y]?; t = T[x]?
  for z in s:
    if z in t:
      output(x,y,z)
\end{lstlisting}
    \caption{A more efficient plan.}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\linewidth}
    \begin{lstlisting}[showlines=true]
# [ R(x, y) | S(y), T(x) ]
# -> [ S(z) $\cap$ T(z) ]

for (x,y) in R:
  s = S[y]?; t = T[x]?
  for z in s $\cap$ t:
    output(x,y,z)

\end{lstlisting}
    \caption{Plan with intersect ($\cap$).}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\linewidth}
    \begin{lstlisting}[showlines=true, numbers=none]
# [ R(y) $\cap$ S(y) ]
# -> [ S(z) $\cap$ T(z) ]
#    -> [ T(x) $\cap$ R(x) ]
for y in R.y $\cap$ S.y:
  for z in S[y] $\cap$ T.z:
    for x in T[z] $\cap$ R[y]:
      output(x,y,z)

\end{lstlisting}
    \caption{Plan equivalent to \GJ.}
  \end{subfigure}
  \caption{Four different \FJ plans for $Q_\triangle$ and their execution.}
\end{figure*}

\subsection{Other Optimizations}\label{sec:other-optimizations}
Lazy trie building, dynamic replanning, Generic Join
Vectorization, query planning.

\subsection{\GJ}\label{sec:background:gj}

\GJ was introduced in~\cite{DBLP:journals/sigmod/NgoRR13} and is the
simplest worst-case optimal join algorithm.  It is based on the
earlier Leapfrog Triejoin algorithm~\cite{DBLP:conf/icdt/Veldhuizen14}.
%
\GJ computes a join query through a series of nested
loops, where each loop iterates over a {\em variable} (not a tuple).
Concretely, \GJ chooses arbitrarily a variable $x$, computes the
intersection of all $x$-columns of all relations containing $x$, and
for each value $a$ in this intersection it computes the residual query
$Q[a/x]$, where every relation $R$ that contains $x$ is replaced with
$\sigma_{x=a}(R)$.  In pseudocode (where $\Pi_x (R_i)$ is the $x$-column of $R_i$):
%
\begin{lstlisting}[basicstyle=\ttfamily]
def join(Q, tuple): # tuple is initially empty
  if Q is empty: print(tuple)
  else:
    for a in $\bigcap_{i : x \in schema(R_i)} \Pi_x(R_i)$:
      join(Q[a/x], tuple ++ [a])
\end{lstlisting}
%
If the query $Q$ has $k$ variables, then there are $k$ nested loops in
\GJ.  In the inner most loop, \GJ outputs the tuple of constants, one
from each iteration.\footnote{For bag semantics, it multiplies their
  multiplicities.}  We notice that a plan for \GJ consists of a total
order of the variables of the query, which we denote as
$[x_1, x_2, \ldots, x_k]$.  Assuming that the intersection above is
done optimally (see below), the algorithm is provably
worst-case-optimal, for any choice of the variable order.

\begin{example}
  Fig.~\ref{fig:background:gj} shows the execution of \GJ on the
  query $Q_\triangle$, using the variable order $[x,y,z]$.  We denoted
  $\Pi_x(R)$ by $R.x$, and denoted (with some abuse) $\sigma_{x=a}(R)$
  by $R[a]$.
\end{example}


While binary joins use hash tables, an implementation of \GJ uses a
\emph{hash trie}, one for each relation in the query.  The hash-trie
is a tree, whose depth is equal to one plus the number of attributes
of the relation, and where each node is either an empty leaf
node,\footnote{For bag semantics, we store in the leaf the
  multiplicity of the tuple.} or a hash map mapping each atomic value
to another node.  We will call the \emph{level} of a node to be the
distance from the root, i.e. the root has level 0, its children level
1, etc.  The hash-trie completely represents the relation: every
root-to-leaf path corresponds precisely to one tuple in the relation.
\GJ uses the hash-trie as follows.  In order to compute
$\sigma_{x=a}(R)$, it simply probes the current hash table for the
value $x=a$, and returns the corresponding child.  To compute an
intersection $\Pi_x(R_1) \cap \Pi_x(R_2) \cap \cdots$, it selects the
trie with the fewest keys, say $R_1$, then iterates over every value
$a$ in the keys for $R_1$ and probes it in each of the hash-maps for
$R_2, R_3, \ldots$; this is a provably optimal algorithm for the
intersection.

\begin{example}
  Consider the query $Q_\triangle$ and the \GJ plan $[x, y, z]$.  We
  first build a hash trie each for $R$, $S$, and $T$.  Each trie has
  three levels including the leaf.  Level 0 of $R$ is keyed on $x$,
  level 1 is keyed on $y$, level 2 contains empty leaf nodes, and
  similarly for $S$ and $T$.  Consider again the pseudocode in
  Figure~\ref{fig:background:gj}.  The first loop intersects level 0
  of the $R$-trie and the $T$-trie.  For each value $a$ in the
  intersection, we retrieve the corresponding children $R[a]$ and
  $T[a]$ respectively; these are at level 1.  The second loop
  intersects the hash map $R[a]$ (at level 1) with the level 0
  hash-map of $S$.  For each value $b$ in the intersection it
  retrieves the corresponding children (levels 2 and 1 respectively),
  and, finally, the innermost loop intersects the $S$- and $T$-hash
  maps (both at level 2), and outputs $(a,b,c)$ for each $c$ in the
  intersection.  So far we have assumed set semantics; if the
  relations have bag semantics, then we simply multiply the tuple
  multiplicities on the leaves (level 3).
\end{example}

%%% The original \GJ algorithm was specified for set semantics,
%%% however~\cite{DBLP:journals/pvldb/FreitagBSKN20} showed the algorithm
%%% can be easily extended to bag semantics.  We therefore ignore the
%%% difference between the two semantics for the rest of this paper.

\subsection{Binary Join v.s. \GJ}
Binary join and \GJ each have their own advantages and disadvantages.
\GJ became popular because of its asymptotic performance guarantee:
Ngo, R{\'{e}}, and Rudra~\cite{DBLP:journals/sigmod/NgoRR13} proved the algorithm is
\emph{worst-case optimal} for \emph{any variable order}, in the sense
that its run time is bounded by the largest possible size of its
output, called AGM bound~\cite{DBLP:journals/siamcomp/AtseriasGM13}.
For example, \GJ executes $Q_\triangle$ in time
$\sqrt{|R|\cdot |S| \cdot |T|}$, which is $n^{3/2}$ when all relations
have size $n$; in contrast, a binary join plan can take $\Omega(n^2)$.
We note, however, that this formula does not include the preprocessing
time needed to construct the tries.  For example, if $T$ is
significantly larger than $R, S$, then the run time of \GJ is
$\ll |T|$, yet during preprocessing \GJ needs to read the entire
relation $T$.  On the other hand, binary join has been a staple of
database systems for decades.  The hash table data structure is
simpler than hash tries and is cheaper to build.  Techniques like
vectorized execution and column-oriented layout have also made binary
join practically efficient, but these optimizations have not been
adapted for \GJ.  Binary join plans are known to be very sensitive to
the choice of the optimizer: poor plans perform catastrophically
bad~\cite{DBLP:journals/pvldb/LeisGMBK015}.  In contrast, although the
runtime performance of \GJ does depend on the variable order, some
researchers believe that \GJ is less sensitive to poor variable
orders, in part because it is always theoretically optimal.


%%% \subsection{Basic Concepts}\label{sec:basic-concepts}
%%% 
%%% First, we define relations under bag semantics:
%%% 
%%% \begin{definition}
%%%   A \emph{relation} is a bag (multiset) of tuples.
%%%   The number of values in a tuple is its \emph{arity}.
%%%   Every tuple in the same relation has the same arity 
%%%     which is also the arity of the relation.
%%%   Each relation has a \emph{schema}, which is a tuple of distinct variables.
%%%   The size of the schema is the same as the arity of the relation.
%%% \end{definition}
%%% 
%%% For convenience, we sometimes refer to the schema of a tuple, 
%%%   which is the schema of the relation holding that tuple.
%%% We also associate each value in a tuple with a variable in the tuple's schema.
%%% 
%%% To simplify presentation we focus on queries involving only 
%%%   natural joins in this paper, although our implementation 
%%%   also supports standard relational algebra operations 
%%%   like selection, projection and aggregation.
%%% In other words, every query is a \emph{full conjunctive query}:
%%% \begin{definition}
%%%   A \emph{full conjunctive query} is a set of \emph{atom}s,
%%%   where each atom is a relation name paired with a tuple of variables.
%%%   The number of variables in each tuple is the same as the arity of the relation.
%%%   We assume no two atoms share the same relation name, 
%%%   and each variable appears at most once in the same atom.
%%% \end{definition}
%%% 
%%% We handle a self-join of a relation $R$
%%%   by creating a copy of $R$ and renaming it to $R'$.
%%% This is the standard approach in database systems 
%%%   that require each query plan to be a tree.
%%% Without loss of generality,
%%%   we assume each relation's schema coincides with 
%%%   the variables of its atom in the query.
%%% 
%%% \begin{example}
%%%   Consider the \emph{triangle query} over relations $R$, $S$, $T$:
%%% $$Q_{\triangle} \cd R(x, y), S(y,z), T(x, z).$$
%%% It is the same as the following SQL query:
%%% \begin{lstlisting}[
%%%     language=SQL,
%%%     showspaces=false,
%%%     basicstyle=\ttfamily\small,
%%%     numbers=left,
%%%     numberstyle=\tiny,
%%%     commentstyle=\color{gray}
%%%  ]
%%% SELECT * FROM R, S, T -- schema: R(x,y), S(y,z), T(x,z)
%%%  WHERE R.y = S.y AND S.z = T.z AND T.x = R.x
%%% \end{lstlisting}
%%% \end{example}
%%% 
%%% The precise definition of cyclic and acyclic queries 
%%%   is not important for this paper,
%%%   so we discuss them informally and briefly here.
%%% Fix a conjunctive query $Q$.
%%% The \emph{query hypergraph} of $Q$ consists of vertices $\mathcal{V}$
%%%   and edges $\mathcal{E}$, 
%%%   where each $v \in \mathcal{V}$ is a variable
%%%   and each hyperedge $(v_1, \ldots, v_k) \in \mathcal{E}$
%%%   is the relation whose atom has the variables $(v_1, \ldots, v_k)$.
%%% A sufficient condition for a query to be acyclic is that 
%%%   the query hypergraph is a tree.
%%% $Q_\triangle$ is a \emph{cyclic} query, 
%%%   whereas any chain query is acyclic, e.g. $Q \cd R(x,y), S(y, z), T(z,w).$
%%% 
%%% \subsection{Binary Join}\label{sec:binary-join}
%%% Now we review relevant concepts for binary join, 
%%%   including its data structures, query plan.
%%% 
%%% Binary join works over \emph{hash tables}:
%%% 
%%% \begin{definition}
%%%   A \emph{hash table} for $R$ is a hash map where each key is a tuple, 
%%%     and every key is mapped to a vector of tuples.
%%% \end{definition}
%%% 
%%% A \emph{binary plan} specifies how to execute a query with binary join:
%%% 
%%% \begin{definition}
%%%   A \emph{binary plan} is a binary tree where each leaf is a distinct relation.
%%%   A \emph{left-deep linear plan} is a binary plan where every right-child of an internal node is a leaf.
%%%   Alternatively, we can write every left-deep linear plan as a sequence of relation names.
%%%   There is a unique leaf that is a left-child, and the relation of that leaf is called the \emph{left relation}.
%%%   A binary plan that is not a left-deep linear plan is called a \emph{bushy plan}.
%%% \end{definition}
%%% 
%%% By this definition, we consider both ``right-deep'' plans and ``zig-zag'' plans as bushy plans,
%%%   and do not distinguish between them.
%%% A standard relational database executes a bushy plan by decomposing it into a series of left-deep linear plans.
%%% That is, for each leaf on the left, it traverses the binary tree bottom-up by going only to the left parent,
%%%   and all the nodes visited form a left-deep linear plan.
%%% Then we execute these plans starting with the right-most one, materializing the intermediate results as we go.
%%% We omit a formal description of this process for brevity.
%%% Our system decomposes bushy plans in exactly the same way,
%%%   and we will focus on left-deep linear plans in the rest of this paper.
%%% 
%%% \begin{example}
%%%   A possible left-deep linear plan for $Q_\triangle$ is $[R, S, T]$.
%%% \end{example}
%%% 
%%% We assume the reader is familiar with how a left-deep linear plan executes,
%%%   and only provide an informal example here.
%%%   of binary join for a different query.
%%% 
%%% \begin{figure}
%%%   \begin{subfigure}[b]{0.45\linewidth}
%%% \begin{lstlisting}
%%% for (x, y) in R:
%%%   S = S[y]?
%%%   for z in S:
%%%     T = T[x,z]?
%%%     for () in T:
%%%       output(x, y, z)
%%% \end{lstlisting}
%%%     \caption{Binary join.}
%%%     \label{fig:back-binary-join}
%%%   \end{subfigure}
%%%   \begin{subfigure}[b]{0.45\linewidth}
%%%     \centering
%%% \begin{lstlisting}
%%% for x in R.x $\cap$ T.x:
%%%   R = R[x]; T = T[x]
%%%   for y in R.y $\cap$ S.y:
%%%     S = S[y]
%%%     for z in S.z $\cap$ T.z:
%%%       output(x, y, z)
%%% \end{lstlisting}
%%%     \caption{\GJ.}
%%%     \label{fig:back-gj}
%%%   \end{subfigure}
%%%   \Description[TODO]{TODO}
%%%   \caption{Execution of binary join and \GJ for $Q_\triangle$.
%%%   The notation \lstinline|S[y]?| performs a lookup on $S$ with the key $y$,
%%%    and continues to the enclosing loop if the lookup fails.  }
%%% \end{figure}
%%% 
%%% \begin{example}
%%%   Given the query $Q_\triangle$ and the plan $[R, S, T]$,
%%%     we first build a hash table for $S$ keyed on $y$,
%%%     where each $y$ maps to a vector of $z$ values.
%%%     Then we build a hash table for $T$ keyed on $x$ and $z$,
%%%     each mapped to a vector of empty tuples\footnote{
%%%       We store the vectors of empty tuples to enforce bag semantics.
%%%       An efficient implementation may simply store a count of duplicates.
%%%     }.
%%%   The nested loops in Figure~\ref{fig:back-binary-join} show the execution 
%%%     of binary join.
%%%   For each tuple $t = (x, y)$ in $R$, 
%%%     we first probe into the hash table for $S$ using $y$
%%%     to get a vector of $z$ values.
%%%   We then loop over each $z$ 
%%%     and probe into the hash table for $T$ using $x$ and $z$. 
%%%   Each successful probe will return a vector of empty tuples, 
%%%     and we output the tuple $(x, y, z)$ for each empty tuple.
%%% \end{example}
%%% 
%%% \subsection{\GJ}
%%% This section reviews basics of \GJ,
%%%   including the hash trie data structure, 
%%%   the \GJ plan, 
%%%   and the execution of \GJ.
%%% 
%%% The data structure \GJ uses is called the \emph{hash trie}, 
%%%   defined recursively as follows:
%%% 
%%% \begin{definition}
%%%   A \emph{hash trie} is either a leaf, 
%%%     or a hash map where each key is a single value 
%%%       and every key is mapped to a hash trie.
%%% \end{definition}
%%% 
%%% A hash trie can be seen as a tree, where 
%%%   the concepts of \emph{parent}, and \emph{child}
%%%   are defined in the usual way.
%%% 
%%% \begin{definition}
%%%  A trie is a \emph{root} if it has no parents;
%%%   any trie that is not a root is a \emph{subtrie}.
%%% The \emph{level} of a subtrie is its distance from the root,
%%%   i.e., the root is at level 0.
%%% \end{definition}
%%% 
%%% A trie can represent a relation, 
%%%   where the $i$-th level of the trie 
%%%   stores values of the $i$-th attribute of the relation.
%%% All the values along each path from the root to a leaf form 
%%%   a tuple in the relation.
%%% 
%%% Now we define the \GJ plan that specify how to execute a query with \GJ.
%%% 
%%% \begin{definition}
%%%   Fix a query $Q$.
%%%   A \GJ plan for $Q$ is a total ordering of the variables in $Q$.
%%% \end{definition}
%%% 
%%% A \GJ plan is executed by a series of nested loops, 
%%%   where each loop level processes one variable 
%%%   by \emph{intersecting} multiple tries on that variable.
%%% We omit a formal definition for brevity, 
%%%   and instead provide an informal example.
%%% 
%%% \begin{example}
%%%   Consider the query $Q_\triangle$ and the \GJ plan $[x, y, z]$.
%%%   We first build a hash trie each for $R$, $S$, and $T$.
%%%   Each trie has three levels including the leaf.
%%%   The first two levels for $R$, $S$, and $T$ are 
%%%     keyed on $(x,y)$, $(y,z)$, and $(x,z)$, respectively.
%%%   The execution of \GJ over these tries is shown in Figure~\ref{fig:back-gj}.
%%%   The first loop intersects the tries for $R$ and $T$ on their keys.
%%%   For each value $x$ in the intersection, 
%%%     we retrieve the subtries mapped to $x$ from $R$ and $T$ respectively.
%%%   The second loop intersects the subtrie for $R$ with the trie for $S$ on $y$,
%%%     and retrieves the subtrie mapped to $y$ from $S$.
%%%   Finally, the innermost loop intersects the subtries for $S$ and $T$ on $z$,
%%%     and outputs the tuple $(x, y, z)$ for each value $z$ in the intersection.
%%% \end{example}
%%% 
%%% The original \GJ algorithm was specified for set semantics, 
%%%   however~\cite{DBLP:journals/pvldb/FreitagBSKN20} showed
%%%   the algorithm can be easily extended to bag semantics.
%%% We therefore ignore the difference between the two semantics 
%%%   for the rest of this paper.
%%% 
%%% \subsection{Binary Join v.s. \GJ}
%%% Binary join and \GJ each have their own advantages and disadvantages.
%%% \GJ became popular because of its asymptotic performance guarantee:
%%%   \citet{DBLP:journals/sigmod/NgoRR13} proved the algorithm is 
%%%   \emph{worst-case optimal}, in the sense that 
%%%   its run time is bounded by the worst-case output size.
%%% For example, if each of the three relations in $Q_\triangle$ has $n$ tuples,
%%%   there can be at most $n^{3/2}$ tuples in the output, 
%%%   and \GJ runs in $O(n^{3/2})$ time.
%%% In contrast, a binary join plan can take $\Omega(n^2)$ time 
%%%   to compute the same query.
%%% On the other hand, binary join has been a staple of database systems for decades. 
%%% The hash table data structure is simpler than hash tries and is cheaper to build.
%%% Techniques like vectorized execution and column-oriented layout have also 
%%%   made binary join practically efficient,
%%%   but these optimizations have not been adapted for \GJ.
%%% 